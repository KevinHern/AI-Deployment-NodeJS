/*
	THIS IS A NODEJS-EXPRESS BACKEND APPLICATION
	You must install 4 mandatory packages:
	1. Express JS (npm install express --save):
		To manage routing and function execution.
	2. Body Parser (npm install body-parser --save):
		To parse JSON, text, Raw and URL encoded data.
	3. Tensorflow JS (npm install @tensorflow/tfjs-node):
		Get the tensorflow library to manage all the AI related stuff.
	4. Base64 to Uint8Array (npm install base64-to-uint8array):
		To decode base64 images
*/

// Imports
const express = require('express');
const tf = require('@tensorflow/tfjs');
const tfnode = require('@tensorflow/tfjs-node');
const fs = require('fs');
const toUint8Array = require('base64-to-uint8array')

// Instantiation
var app = express();
app.use(express.json()); // To be able to read and write JSONs

/* ----- FUNCTIONS ----- */
// IMPORTANT: the function that loads and calls the model must be an async function!
async function runModel(){
	/* ----- AI Section ----- */

	// 1. Provide the PATH to the model.json after the 'file://'
	const modelPath = 'file://assets/ai/model.json';	

	// 2. Load the model and await it
	const model = await tf.loadLayersModel(modelPath);

	// 2.1 When manipulating tensors and calling .predict on a model, wrap it up in a tf.tidy() function
	// This function frees the resources used when pre processing tensors.
	// Memory management is critical!
	const results = tf.tidy(() => {
		// 2.2 Generating a noise vector
		const noiseVector = tf.randomNormal([1, 100]);

		// 3. Call the predict function. In this case, we don't want to call '.dataSync()' yet, 
		// we still have to modify the tensor generated by the model
		var results = model.predict([noiseVector]);
		results = results.mul(127.5).add(127.5);
		return results.reshape([results.shape[1], results.shape[2], results.shape[3]]);
	});

	// 4. Do stuff with the output tensor
	// (In this case, we want to return the encoded generated image in base 64)
	const newImg = await tfnode.node.encodeJpeg(results, format='grayscale');
	return Buffer.from(newImg).toString('base64');
}

// This is the main URL to do predictions
app.get('/predict', async function (req, res) {
	try{
		// Run the Model and be sure to await it
		results = await runModel();
		res.json({generated_img: results});
	}
	catch(error){
		console.log(error);
		res.sendStatus(500);
	}
})

// Route used for redirection if another route is attempted to get accesed
app.get('/*', function(req, res) {
	res.send('Hello :D');
});

// Route used for redirection if another route is attempted to get accesed
app.post('/*', function(req, res) {
	res.send('Hello :D');
});

// Instantiating the server
/*
	NOTE: There are 2 ports that the app can liston to.
	The process.env.PORT is port dynamically allocated to the app when it has been deployed on a production
	environment.
	The other port is a user assigned port for developing. This port only works locally
*/
var server = app.listen(process.env.PORT || 16000, function () {
   var host = server.address().address
   var port = server.address().port
   
   console.log("Example app listening at http://%s:%s", host, port)
})